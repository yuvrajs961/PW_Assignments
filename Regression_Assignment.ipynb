{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T7PVp-dmm71X"
      },
      "outputs": [],
      "source": [
        "# Question 1: What is Simple Linear Regression?\n",
        "# Answer:\n",
        "# Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X). The relationship is modeled by fitting a linear equation Y = mX + c, where m is the slope and c is the intercept.\n",
        "\n",
        "# Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "# Answer:\n",
        "# - Linearity: The relationship between X and Y is linear.\n",
        "# - Independence: Observations are independent of each other.\n",
        "# - Homoscedasticity: Constant variance of errors.\n",
        "# - Normality: Residuals are normally distributed.\n",
        "# - No multicollinearity (only one predictor, so not applicable here).\n",
        "\n",
        "# Question 3: What does the coefficient m represent in the equation Y = mX + c?\n",
        "# Answer:\n",
        "# The coefficient m is the slope of the line. It represents the change in the dependent variable Y for a one-unit change in the independent variable X.\n",
        "\n",
        "# Question 4: What does the intercept c represent in the equation Y = mX + c?\n",
        "# Answer:\n",
        "# The intercept c is the expected value of Y when X = 0. It indicates where the regression line crosses the Y-axis.\n",
        "\n",
        "# Question 5: How do we calculate the slope m in Simple Linear Regression?\n",
        "# Answer:\n",
        "# The slope m is calculated using the formula:\n",
        "# m = Σ[(Xi - X̄)(Yi - Ȳ)] / Σ[(Xi - X̄)^2]\n",
        "\n",
        "# Question 6: What is the purpose of the least squares method in Simple Linear Regression?\n",
        "# Answer:\n",
        "# The least squares method minimizes the sum of the squared differences between the observed values and the predicted values. It finds the best-fitting line through the data.\n",
        "\n",
        "# Question 7: How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "# Answer:\n",
        "# R² measures the proportion of variance in the dependent variable that is explained by the independent variable. An R² of 0.8 means 80% of the variability in Y is explained by X.\n",
        "\n",
        "# Question 8: What is Multiple Linear Regression?\n",
        "# Answer:\n",
        "# Multiple Linear Regression models the relationship between a dependent variable and two or more independent variables using the equation: Y = b0 + b1X1 + b2X2 + ... + bnXn.\n",
        "\n",
        "# Question 9: What is the main difference between Simple and Multiple Linear Regression?\n",
        "# Answer:\n",
        "# Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "# Question 10: What are the key assumptions of Multiple Linear Regression?\n",
        "# Answer:\n",
        "# - Linearity\n",
        "# - Independence\n",
        "# - Homoscedasticity\n",
        "# - Normality of residuals\n",
        "# - No or minimal multicollinearity among independent variables\n",
        "\n",
        "# Question 11: What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "# Answer:\n",
        "# Heteroscedasticity means that the variance of the errors is not constant across all levels of the independent variables. It can lead to inefficient estimates and biased inference.\n",
        "\n",
        "# Question 12: How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "# Answer:\n",
        "# - Remove or combine correlated variables\n",
        "# - Use regularization techniques (e.g., Ridge or Lasso)\n",
        "# - Use Principal Component Analysis (PCA)\n",
        "\n",
        "# Question 13: What are some common techniques for transforming categorical variables for use in regression models?\n",
        "# Answer:\n",
        "# - One-hot encoding\n",
        "# - Label encoding (for ordinal variables)\n",
        "# - Binary encoding or frequency encoding\n",
        "\n",
        "# Question 14: What is the role of interaction terms in Multiple Linear Regression?\n",
        "# Answer:\n",
        "# Interaction terms model the effect of two variables interacting with each other on the dependent variable. For example, X1*X2 captures how the effect of X1 changes at different levels of X2.\n",
        "\n",
        "# Question 15: How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "# Answer:\n",
        "# In Simple Linear Regression, the intercept is the value of Y when X = 0. In Multiple Linear Regression, it is the value of Y when all independent variables are 0.\n",
        "\n",
        "# Question 16: What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "# Answer:\n",
        "# The slope indicates the magnitude and direction of the effect of an independent variable on the dependent variable. Larger slopes have greater influence on predictions.\n",
        "\n",
        "# Question 17: How does the intercept in a regression model provide context for the relationship between variables?\n",
        "# Answer:\n",
        "# The intercept anchors the regression line. It tells us the starting value of the dependent variable when all predictors are zero.\n",
        "\n",
        "# Question 18: What are the limitations of using R² as a sole measure of model performance?\n",
        "# Answer:\n",
        "# R² does not indicate whether a model is appropriate or whether predictors are statistically significant. It can be artificially inflated by adding variables.\n",
        "\n",
        "# Question 19: How would you interpret a large standard error for a regression coefficient?\n",
        "# Answer:\n",
        "# A large standard error indicates that the coefficient estimate is imprecise and the variable may not be a significant predictor.\n",
        "\n",
        "# Question 20: How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "# Answer:\n",
        "# In residual plots, heteroscedasticity appears as a fan or funnel shape. It's important to address because it violates regression assumptions and affects inference validity.\n",
        "\n",
        "# Question 21: What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "# Answer:\n",
        "# It suggests that some predictors may not contribute to the model. Adjusted R² accounts for the number of predictors and penalizes non-useful ones.\n",
        "\n",
        "# Question 22: Why is it important to scale variables in Multiple Linear Regression?\n",
        "# Answer:\n",
        "# Scaling ensures all variables contribute equally to the model, especially when using regularization techniques. It also helps in faster convergence during optimization.\n",
        "\n",
        "# Question 23: What is polynomial regression?\n",
        "# Answer:\n",
        "# Polynomial regression is an extension of linear regression where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "# Question 24: How does polynomial regression differ from linear regression?\n",
        "# Answer:\n",
        "# Polynomial regression fits a nonlinear curve to the data by adding higher-degree terms (e.g., X², X³), while linear regression fits a straight line.\n",
        "\n",
        "# Question 25: When is polynomial regression used?\n",
        "# Answer:\n",
        "# It is used when the data shows a curvilinear relationship between the variables, which cannot be captured by a straight line.\n",
        "\n",
        "# Question 26: What is the general equation for polynomial regression?\n",
        "# Answer:\n",
        "# Y = b0 + b1X + b2X² + b3X³ + ... + bnXⁿ\n",
        "\n",
        "# Question 27: Can polynomial regression be applied to multiple variables?\n",
        "# Answer:\n",
        "# Yes, multivariate polynomial regression can be used, involving polynomial terms for more than one independent variable (e.g., X1², X1*X2, X2³).\n",
        "\n",
        "# Question 28: What are the limitations of polynomial regression?\n",
        "# Answer:\n",
        "# - Overfitting with high-degree polynomials\n",
        "# - Difficult to interpret coefficients\n",
        "# - Sensitive to outliers\n",
        "\n",
        "# Question 29: What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "# Answer:\n",
        "#- Cross-validation\n",
        "# - Adjusted R²\n",
        "# - AIC/BIC\n",
        "# - Residual plots\n",
        "\n",
        "# Question 30: Why is visualization important in polynomial regression?\n",
        "#Answer: Visualization helps to understand the fit of the model, detect overfitting/underfitting, and interpret the shape of the curve.\n",
        "\n",
        "# Question 31: How is polynomial regression implemented in Python?\n",
        "# Answer:\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "print(\"Prediction for 6:\", model.predict([[6]]))\n"
      ]
    }
  ]
}