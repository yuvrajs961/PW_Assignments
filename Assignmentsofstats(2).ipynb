{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RljGGK8w4DcE"
      },
      "outputs": [],
      "source": [
        "#Question:  What is hypothesis testing in statistics?\n",
        "#Answer: Hypothesis testing is a fundamental procedure in statistics used to make inferences about populations based on sample data. It involves forming two competing hypotheses: the null hypothesis (H₀) and the alternative hypothesis (H₁). The null hypothesis typically represents a statement of no effect or no difference in the population, whereas the alternative hypothesis suggests that there is an effect or difference. By using sample data, statistical methods, and probability theory, hypothesis testing helps us decide whether to reject the null hypothesis in favor of the alternative hypothesis. This process involves comparing observed data to what is expected under the null hypothesis to determine if any deviations are statistically significant. The test provides a decision-making framework that helps researchers assess the validity of their assumptions about the population.\n",
        "#Question:  What is the null hypothesis, and how does it differ from the alternative hypothesis?\n",
        "#Answer: The null hypothesis (H₀) is a statement of no effect, no difference, or no relationship between variables in a population. It essentially posits that any observed effect or relationship is due to chance or random variation. The null hypothesis is assumed to be true unless evidence suggests otherwise. On the other hand, the alternative hypothesis (H₁ or Ha) is a statement that contradicts the null hypothesis. It suggests that there is an effect, a difference, or a relationship between variables. The alternative hypothesis represents the researcher's belief or the direction of the expected outcome, such as a difference in means, a correlation, or a causal relationship. In hypothesis testing, the null hypothesis is tested to determine whether there is enough evidence to reject it in favor of the alternative hypothesis.\n",
        "#Question:  What is the significance level in hypothesis testing, and why is it important?\n",
        "#Answer: The significance level (α) is a critical threshold used in hypothesis testing to decide whether the observed data provides enough evidence to reject the null hypothesis. It represents the probability of making a Type I error, which is the false rejection of the null hypothesis when it is actually true. The significance level is typically set at 0.05 (5%), meaning there is a 5% chance of incorrectly rejecting the null hypothesis. This level defines the \"risk\" researchers are willing to take in making such an error. The significance level is important because it controls for the likelihood of making incorrect conclusions based on the sample data. A smaller significance level (e.g., 0.01) reduces the risk of Type I errors but may increase the likelihood of Type II errors (failing to reject a false null hypothesis).\n",
        "#Question:  What does a P-value represent in hypothesis testing?\n",
        "#Answer: A P-value is a probability that measures the strength of the evidence against the null hypothesis. It represents the probability of obtaining a test statistic at least as extreme as the one observed in the sample, assuming the null hypothesis is true. In other words, the P-value quantifies the likelihood of observing the data (or something more extreme) given that the null hypothesis holds. A smaller P-value indicates stronger evidence against the null hypothesis, suggesting that the observed results are less likely to occur under the assumption of no effect. Researchers typically compare the P-value to the significance level (α) to make a decision about the null hypothesis. If the P-value is less than or equal to α, the null hypothesis is rejected, indicating that the data provide sufficient evidence to support the alternative hypothesis.\n",
        "#Question:  How do you interpret the P-value in hypothesis testing?\n",
        "#Answer: The interpretation of the P-value depends on its comparison to the significance level (α). If the P-value is smaller than or equal to the chosen significance level, it suggests that the observed data is unlikely under the null hypothesis, and there is enough evidence to reject the null hypothesis. In this case, we conclude that the alternative hypothesis is more likely. For example, if α is set at 0.05 and the P-value is 0.03, this means that there is a 3% probability of observing the data (or something more extreme) assuming the null hypothesis is true. Since 0.03 is less than 0.05, we reject the null hypothesis. If the P-value is greater than the significance level (e.g., P-value = 0.08 and α = 0.05), we fail to reject the null hypothesis because the data does not provide sufficient evidence to support the alternative hypothesis. It is important to note that a P-value is not a direct measure of the probability that the null hypothesis is true or false.\n",
        "#Question:  What are Type 1 and Type 2 errors in hypothesis testing?\n",
        "#Answer: In hypothesis testing, errors can occur in two forms: Type 1 and Type 2. A Type 1 error occurs when the null hypothesis is rejected even though it is actually true. This is also known as a false positive. For example, a Type 1 error occurs if a medical test incorrectly concludes that a patient has a disease when they do not. The probability of a Type 1 error is denoted by the significance level (α), which is the threshold for rejecting the null hypothesis. A Type 2 error occurs when the null hypothesis is not rejected even though it is false. This is also called a false negative. For instance, a Type 2 error might occur if a test fails to detect a disease when the patient actually has it. The probability of a Type 2 error is denoted by β, and the power of a test is the probability of correctly rejecting the null hypothesis, which is equal to 1 - β. Balancing the risks of Type 1 and Type 2 errors is crucial in hypothesis testing.\n",
        "#Question:  What is the difference between a one-tailed and a two-tailed test in hypothesis testing?\n",
        "#Answer: A one-tailed test is used when the researcher is testing for the possibility of an effect or difference in only one direction, either greater than or less than a certain value. For example, if a researcher hypothesizes that a new drug is more effective than a standard drug, they would use a one-tailed test to examine if the drug's effect is significantly greater. In contrast, a two-tailed test tests for the possibility of an effect or difference in both directions—either greater than or less than a certain value. This type of test is used when there is no prior expectation about the direction of the effect. For example, testing whether a new drug is either more effective or less effective than a standard drug would require a two-tailed test. The main difference lies in the directionality of the test; one-tailed tests focus on one specific outcome, while two-tailed tests account for both possible outcomes.\n",
        "#Question:  What is the Z-test, and when is it used in hypothesis testing?\n",
        "#Answer: The Z-test is a statistical test used to determine whether there is a significant difference between sample data and population parameters (such as the population mean) when the population standard deviation is known, and the sample size is large (usually greater than 30). It is commonly used when the data follows a normal distribution, and the population variance is known or can be reasonably estimated. The Z-test involves calculating a Z-score, which represents the number of standard deviations the sample mean is away from the population mean. The Z-test is used in situations such as comparing sample means to population means, testing proportions, and assessing the goodness of fit of observed data to expected data under a null hypothesis.\n",
        "#Question: How do you calculate the Z-score, and what does it represent in hypothesis testing?\n",
        "#Answer: The Z-score is calculated by subtracting the population mean (μ) from the sample mean (X̄) and dividing the result by the population standard deviation (σ). The formula is: Z=Xˉ−μσZ = \\frac{X̄ - μ}{σ}Z=σXˉ−μ . The Z-score represents how many standard deviations the sample mean is from the population mean. It provides a standardized value that allows comparison of data points across different scales or distributions. In hypothesis testing, the Z-score is used to determine the likelihood of observing a sample mean given a population mean and standard deviation. A Z-score is critical in finding the probability associated with a sample statistic under the normal distribution, and it helps decide whether the observed data supports or contradicts the null hypothesis.\n",
        "#Question: What is the T-distribution, and when should it be used instead of the normal distribution?\n",
        "#Answer: The T-distribution is a probability distribution that is similar to the normal distribution but has heavier tails. It is used in hypothesis testing when the sample size is small (typically n < 30) and the population standard deviation is unknown. The T-distribution is more appropriate than the normal distribution for small samples because it accounts for the increased variability in sample data. As the sample size increases, the T-distribution approaches the normal distribution. The T-distribution is essential for tests like the T-test, which compares the means of small samples to population means. The shape of the T-distribution depends on the sample size, with larger samples leading to a distribution closer to the normal distribution.\n",
        "#Question: What is the difference between a Z-test and a T-test?\n",
        "#Answer: The Z-test and T-test are both used to test hypotheses about population means, but they differ in their assumptions and when they are appropriate to use. A Z-test is used when the sample size is large (typically n > 30), and the population standard deviation is known or can be reasonably estimated. It assumes the data follows a normal distribution. On the other hand, the T-test is used when the sample size is small (n < 30) or the population standard deviation is unknown. The T-test uses the sample standard deviation in place of the population standard deviation and accounts for the additional uncertainty introduced by small sample sizes. The T-test is more flexible and robust for small datasets, while the Z-test is used for large datasets where the population standard deviation is known.\n",
        "#Question: What is the T-test, and how is it used in hypothesis testing?\n",
        "#Answer: The T-test is a statistical test used to compare the means of two groups or a sample mean to a population mean, especially when the sample size is small or the population standard deviation is unknown. It assumes that the data is approximately normally distributed. There are different types of T-tests: A one-sample T-test compares the mean of a sample to a known population mean. An independent two-sample T-test compares the means of two independent groups. A paired sample T-test compares the means of the same group at two different points in time or under two different conditions. The T-test calculates a T-statistic, which is then compared to a critical value from the T-distribution to determine whether the observed difference is statistically significant.\n",
        "#Question: What is the relationship between Z-test and T-test in hypothesis testing?\n",
        "#Answer: Both the Z-test and T-test are used to compare means and test hypotheses, but they differ in their application based on sample size and knowledge of population parameters. The Z-test is used when the sample size is large and the population standard deviation is known, while the T-test is used for small samples or when the population standard deviation is unknown. As the sample size increases, the T-distribution (used in the T-test) approaches the normal distribution, making the Z-test more appropriate for larger samples. The relationship between the Z-test and T-test is that the T-test becomes similar to the Z-test as the sample size increases, with both tests assessing the same types of hypotheses but using different distributions.\n",
        "#Question: What is a confidence interval, and how is it used to interpret statistical results?\n",
        "#Answer: A confidence interval is a range of values that is used to estimate a population parameter, such as the population mean or proportion. It provides an interval estimate with a certain level of confidence (e.g., 95%) that the true population parameter lies within that interval. The width of the confidence interval depends on the variability of the sample data and the sample size. A narrow confidence interval indicates a precise estimate, while a wide interval suggests more uncertainty. Confidence intervals are used to give a range of plausible values for the parameter, rather than a single point estimate, providing more useful information about the population.\n",
        "#Question:  What is the margin of error, and how does it affect the confidence interval?\n",
        "#Answer: The margin of error represents the amount of uncertainty or variability in an estimate. It is the range within which the true population parameter is likely to fall, given the sample data. The margin of error is influenced by the sample size, the variability in the data, and the desired confidence level. A larger sample size reduces the margin of error and provides a more precise estimate of the population parameter. The margin of error directly affects the width of the confidence interval; a larger margin of error results in a wider interval, indicating less precision in the estimate.\n",
        "#Question: How is Bayes' Theorem used in statistics, and what is its significance?\n",
        "#Answer: Bayes' Theorem is a fundamental concept in probability theory and statistics that allows for the updating of probabilities based on new evidence. It is used to calculate the posterior probability of an event, given prior knowledge and new data. The theorem combines the prior probability (the initial belief about the likelihood of an event) with the likelihood of the observed data to produce a more refined estimate of the event's probability. Bayes' Theorem is significant because it provides a framework for incorporating both prior information and new data to improve decision-making and inference. It is widely used in fields like machine learning, medical diagnostics, and decision analysis.\n",
        "#Question: What is the Chi-square distribution, and when is it used?\n",
        "#Answer: The Chi-square distribution is a probability distribution that is used in hypothesis testing for categorical data. It is commonly used in tests of independence, goodness-of-fit tests, and tests of homogeneity. The Chi-square distribution is based on the sum of squared standard normal variables, and it is skewed to the right, with its shape depending on the degrees of freedom. The distribution is used to assess how well observed frequencies match expected frequencies under the null hypothesis. It is often used to test whether there is a significant association between two categorical variables or whether observed data fits a hypothesized distribution.\n",
        "#Question: What is the Chi-square goodness of fit test, and how is it applied?\n",
        "#Answer: The Chi-square goodness of fit test is used to determine whether a sample data set fits a specific distribution or pattern. It compares the observed frequencies of categories in a sample to the expected frequencies based on a hypothesized distribution. The test calculates a Chi-square statistic, which measures the discrepancy between the observed and expected frequencies. If the Chi-square statistic is significantly large (as determined by comparing it to the critical value from the Chi-square distribution), the null hypothesis is rejected, indicating that the observed data does not fit the expected distribution. This test is commonly applied to categorical data, such as in surveys, polls, and experiments.\n",
        "#Question: What is the F-distribution, and when is it used in hypothesis testing?\n",
        "#Answer: The F-distribution is a probability distribution that arises when comparing variances. It is used in hypothesis testing for comparing the variances of two or more groups. The F-distribution is skewed to the right and depends on two parameters: the degrees of freedom for the numerator and denominator. It is commonly used in analysis of variance (ANOVA) tests, regression analysis, and tests of homogeneity. In these cases, the F-distribution helps assess whether the variances of different groups are significantly different from each other. A significant F-statistic suggests that the group means differ, indicating that one or more factors may be influencing the outcome.\n",
        "#Question: What is an ANOVA test, and what are its assumptions?\n",
        "#Answer: ANOVA (Analysis of Variance) is a statistical test used to determine if there are significant differences between the means of three or more groups. It tests whether the variation between group means is greater than the variation within the groups. The key assumptions of ANOVA are: Independence of observations: Each sample should be independent of the others. Normality: The data within each group should be approximately normally distributed. Homogeneity of variances: The variance within each group should be roughly equal. ANOVA is used to compare group means when there is more than one group, and it allows researchers to assess whether any of the groups differ significantly from the others.\n",
        "#Question: What are the different types of ANOVA tests?\n",
        "#Answer: The main types of ANOVA tests are: One-way ANOVA: Compares the means of three or more independent groups based on one factor. Two-way ANOVA: Compares the means of two or more groups based on two factors, allowing for the examination of interactions between the factors. Repeated measures ANOVA: Compares the means of the same group under different conditions or over multiple time points. This type of ANOVA is used when the same participants are measured multiple times, controlling for within-subject variation.\n",
        "#Question: What is the F-test, and how does it relate to hypothesis testing?\n",
        "#Answer: The F-test is used to compare two or more variances to assess whether they are significantly different. It is commonly used in ANOVA to test whether the variation between group means is larger than the variation within groups, indicating that the group means differ significantly. The F-test involves calculating an F-statistic, which is the ratio of the variance between groups to the variance within groups. If the F-statistic is large enough (compared to the critical value from the F-distribution), the null hypothesis of equal variances is rejected. The F-test is essential for testing hypotheses about variance and is a core component of many statistical models.\n",
        "\n",
        "\n",
        "# Question:  Write a Python program to perform a Z-test for comparing a sample mean to a known population mean and interpret the results.\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample data\n",
        "sample_data = np.array([25, 30, 35, 40, 45, 50])\n",
        "sample_mean = np.mean(sample_data)\n",
        "sample_size = len(sample_data)\n",
        "sample_std = np.std(sample_data, ddof=1)\n",
        "\n",
        "# Population mean\n",
        "population_mean = 40\n",
        "\n",
        "# Z-test calculation\n",
        "z_score = (sample_mean - population_mean) / (sample_std / np.sqrt(sample_size))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")\n",
        "# Question: Simulate random data to perform hypothesis testing and calculate the corresponding P-value using Python.\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "\n",
        "# Perform a one-sample t-test\n",
        "t_stat, p_value = stats.ttest_1samp(data, 55)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from 55.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from 55.\")\n",
        "\n",
        "# Question: Implement a one-sample Z-test using Python to compare the sample mean with the population mean.\n",
        "z_score = (sample_mean - population_mean) / (sample_std / np.sqrt(sample_size))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
        "\n",
        "# Question: Perform a two-tailed Z-test using Python and visualize the decision region on a plot.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate Z-value\n",
        "z_score = (sample_mean - population_mean) / (sample_std / np.sqrt(sample_size))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "# Plotting the decision region\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = stats.norm.pdf(x)\n",
        "\n",
        "plt.plot(x, y, label='Standard Normal Distribution')\n",
        "plt.axvline(x=-1.96, color='red', linestyle='--')\n",
        "plt.axvline(x=1.96, color='red', linestyle='--')\n",
        "plt.fill_between(x, 0, y, where=((x <= -1.96) | (x >= 1.96)), color='red', alpha=0.3)\n",
        "plt.title('Two-Tailed Z-test Decision Region')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if abs(z_score) > 1.96:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from the population mean.\")\n",
        "\n",
        "# Question: Create a Python function that calculates and visualizes Type 1 and Type 2 errors during hypothesis testing.\n",
        "def plot_errors(alpha=0.05, beta=0.2):\n",
        "    # Generate normal distribution curves for H0 and H1\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "    y0 = stats.norm.pdf(x, loc=0, scale=1)  # H0\n",
        "    y1 = stats.norm.pdf(x, loc=1, scale=1)  # H1\n",
        "\n",
        "    # Calculate Type I and Type II errors\n",
        "    critical_value = stats.norm.ppf(1 - alpha)\n",
        "    type1_error = 1 - stats.norm.cdf(critical_value)\n",
        "    type2_error = stats.norm.cdf(critical_value, loc=1)  # Assuming alternative hypothesis\n",
        "\n",
        "    # Plotting\n",
        "    plt.plot(x, y0, label=\"H0 (Null Hypothesis)\")\n",
        "    plt.plot(x, y1, label=\"H1 (Alternative Hypothesis)\")\n",
        "    plt.fill_between(x, 0, y0, where=(x > critical_value), color='red', alpha=0.3, label=\"Type I Error\")\n",
        "    plt.fill_between(x, 0, y1, where=(x < critical_value), color='blue', alpha=0.3, label=\"Type II Error\")\n",
        "    plt.title(f\"Type I and Type II Errors (alpha={alpha}, beta={beta})\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_errors()\n",
        "\n",
        "# Question: Write a Python program to perform an independent T-test and interpret the results.\n",
        "sample1 = np.array([25, 30, 35, 40, 45])\n",
        "sample2 = np.array([30, 35, 40, 45, 50])\n",
        "\n",
        "# Perform independent T-test\n",
        "t_stat, p_value = stats.ttest_ind(sample1, sample2)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The two samples have significantly different means.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The two samples do not have significantly different means.\")\n",
        "\n",
        "# Question: Perform a paired sample T-test using Python and visualize the comparison results.\n",
        "before = np.array([25, 30, 35, 40, 45])\n",
        "after = np.array([30, 35, 40, 45, 50])\n",
        "\n",
        "# Perform paired T-test\n",
        "t_stat, p_value = stats.ttest_rel(before, after)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Visualization\n",
        "plt.boxplot([before, after], labels=[\"Before\", \"After\"])\n",
        "plt.title('Paired Sample T-test')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The means of the paired samples are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The means of the paired samples are not significantly different.\")\n",
        "# Question: Simulate data and perform both Z-test and T-test, then compare the results using Python.\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "\n",
        "# Z-test\n",
        "z_score = (np.mean(data) - 55) / (np.std(data, ddof=1) / np.sqrt(len(data)))\n",
        "p_value_z = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "# T-test\n",
        "t_stat, p_value_t = stats.ttest_1samp(data, 55)\n",
        "\n",
        "print(f\"Z-test: Z-score = {z_score}, P-value = {p_value_z}\")\n",
        "print(f\"T-test: T-statistic = {t_stat}, P-value = {p_value_t}\")\n",
        "\n",
        "# Comparison of results\n",
        "if p_value_z < 0.05 and p_value_t < 0.05:\n",
        "    print(\"Both tests suggest rejecting the null hypothesis.\")\n",
        "elif p_value_z >= 0.05 and p_value_t >= 0.05:\n",
        "    print(\"Both tests suggest failing to reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"The results are conflicting between the two tests.\")\n",
        "\n",
        "# Question: Write a Python function to calculate the confidence interval for a sample mean and explain its significance.\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    mean = np.mean(data)\n",
        "    std_err = stats.sem(data)\n",
        "    margin_of_error = std_err * stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
        "\n",
        "    return mean - margin_of_error, mean + margin_of_error\n",
        "\n",
        "# Example data\n",
        "data = np.array([25, 30, 35, 40, 45])\n",
        "lower, upper = confidence_interval(data)\n",
        "\n",
        "print(f\"Confidence Interval: ({lower}, {upper})\")\n",
        "\n",
        "# Question: Write a Python program to calculate the margin of error for a given confidence level using sample data.\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def margin_of_error(data, confidence=0.95):\n",
        "    mean = np.mean(data)\n",
        "    std_err = stats.sem(data)  # Standard error of the mean\n",
        "    margin = std_err * stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
        "    return margin\n",
        "\n",
        "# Sample data\n",
        "data = [25, 30, 35, 40, 45]\n",
        "margin = margin_of_error(data)\n",
        "print(f\"Margin of Error: {margin}\")\n",
        "\n",
        "# Question: Implement a Bayesian inference method using Bayes' Theorem in Python and explain the process.\n",
        "def bayes_theorem(prior, likelihood, evidence):\n",
        "    return (likelihood * prior) / evidence\n",
        "\n",
        "# Example: Let's calculate the probability of disease (P(Disease|Positive)) using Bayes' theorem\n",
        "prior = 0.01  # Probability of having the disease (P(Disease))\n",
        "likelihood = 0.95  # Probability of testing positive given the disease (P(Positive|Disease))\n",
        "evidence = 0.05  # Probability of testing positive (P(Positive))\n",
        "\n",
        "posterior = bayes_theorem(prior, likelihood, evidence)\n",
        "print(f\"Posterior Probability (P(Disease|Positive)): {posterior}\")\n",
        "\n",
        "# Question: Perform a Chi-square test for independence between two categorical variables in Python.\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Example contingency table for two categorical variables\n",
        "observed = [[30, 10], [20, 40]]  # Rows: Group A and Group B, Columns: Outcome 1 and Outcome 2\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2_stat, p_value, dof, expected = stats.chi2_contingency(observed)\n",
        "\n",
        "print(f\"Chi2 Stat: {chi2_stat}, P-value: {p_value}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(f\"Expected Frequencies: \\n{expected}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The two variables are dependent.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The two variables are independent.\")\n",
        "\n",
        "# Question: Write a Python program to calculate the expected frequencies for a Chi-square test based on observed data.\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Example observed frequencies\n",
        "observed = [[30, 10], [20, 40]]\n",
        "\n",
        "# Calculate expected frequencies\n",
        "_, _, _, expected = stats.chi2_contingency(observed)\n",
        "\n",
        "print(f\"Expected Frequencies: \\n{expected}\")\n",
        "\n",
        "# Question: Perform a goodness-of-fit test using Python to compare the observed data to an expected distribution.\n",
        "observed = [10, 20, 30, 40]\n",
        "expected = [25, 25, 25, 25]\n",
        "\n",
        "# Perform Chi-square goodness-of-fit test\n",
        "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "print(f\"Chi2 Stat: {chi2_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The observed data significantly differs from the expected distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The observed data does not significantly differ from the expected distribution.\")\n",
        "# Question: Create a Python script to simulate and visualize the Chi-square distribution and discuss its characteristics.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Simulating data for Chi-square distribution\n",
        "df = 5  # Degrees of freedom\n",
        "data = np.random.chisquare(df, 1000)\n",
        "\n",
        "# Plotting the Chi-square distribution\n",
        "plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
        "\n",
        "# Plotting the theoretical Chi-square distribution\n",
        "x = np.linspace(0, 20, 1000)\n",
        "y = stats.chi2.pdf(x, df)\n",
        "plt.plot(x, y, 'r-', label='Chi-square distribution')\n",
        "\n",
        "plt.title(f\"Chi-square Distribution with {df} Degrees of Freedom\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Question: Implement an F-test using Python to compare the variances of two random samples.\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Two random samples\n",
        "sample1 = np.random.normal(loc=10, scale=2, size=100)\n",
        "sample2 = np.random.normal(loc=12, scale=3, size=100)\n",
        "\n",
        "# Perform F-test\n",
        "f_stat, p_value = stats.levene(sample1, sample2)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n",
        "\n",
        "# Question: Write a Python program to perform an ANOVA test to compare means between multiple groups and interpret the results.\n",
        "group1 = np.random.normal(loc=20, size=30)\n",
        "group2 = np.random.normal(loc=22, size=30)\n",
        "group3 = np.random.normal(loc=25, size=30)\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The means of the groups are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The means of the groups are not significantly different.\")\n",
        "\n",
        "# Question: Perform a one-way ANOVA test using Python to compare the means of different groups and plot the results.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Multiple groups\n",
        "group1 = np.random.normal(loc=20, size=30)\n",
        "group2 = np.random.normal(loc=22, size=30)\n",
        "group3 = np.random.normal(loc=25, size=30)\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "# Plotting the data\n",
        "plt.boxplot([group1, group2, group3], labels=[\"Group 1\", \"Group 2\", \"Group 3\"])\n",
        "plt.title(f\"ANOVA Test Results: F-statistic = {f_stat}, P-value = {p_value}\")\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The means of the groups are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The means of the groups are not significantly different.\")\n",
        "\n",
        "# Question: Write a Python function to check the assumptions (normality, independence, and equal variance) for ANOVA.\n",
        "from scipy.stats import shapiro, levene\n",
        "\n",
        "def check_anova_assumptions(groups):\n",
        "    # Check normality with Shapiro-Wilk test\n",
        "    normality_results = [shapiro(group) for group in groups]\n",
        "    print(\"Normality test results:\")\n",
        "    for i, result in enumerate(normality_results, 1):\n",
        "        print(f\"Group {i}: W-statistic = {result[0]}, P-value = {result[1]}\")\n",
        "\n",
        "    # Check equality of variances with Levene's test\n",
        "    _, p_value = levene(*groups)\n",
        "    print(f\"Levene's test for equality of variances: P-value = {p_value}\")\n",
        "\n",
        "# Sample groups\n",
        "group1 = np.random.normal(loc=20, size=30)\n",
        "group2 = np.random.normal(loc=22, size=30)\n",
        "group3 = np.random.normal(loc=25, size=30)\n",
        "\n",
        "check_anova_assumptions([group1, group2, group3])\n",
        "\n",
        "# Question: Perform a two-way ANOVA test using Python to study the interaction between two factors and visualize the results.\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = pd.DataFrame({\n",
        "    'Factor1': np.repeat(['A', 'B'], 10),\n",
        "    'Factor2': np.tile(['X', 'Y'], 10),\n",
        "    'Response': np.random.normal(size=20)\n",
        "})\n",
        "\n",
        "# Two-way ANOVA\n",
        "model = ols('Response ~ C(Factor1) + C(Factor2) + C(Factor1):C(Factor2)', data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(anova_table)\n",
        "\n",
        "# Question: Write a Python program to visualize the F-distribution and discuss its use in hypothesis testing.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Visualizing the F-distribution\n",
        "dfn = 2  # Degrees of freedom for numerator\n",
        "dfd = 5  # Degrees of freedom for denominator\n",
        "x = np.linspace(0, 5, 1000)\n",
        "y = stats.f.pdf(x, dfn, dfd)\n",
        "\n",
        "plt.plot(x, y, label='F-distribution')\n",
        "plt.title(f'F-distribution (dfn={dfn}, dfd={dfd})')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# The F-distribution is used in the context of comparing variances or performing ANOVA tests.\n",
        "\n",
        "# Question: Perform a one-way ANOVA test in Python and visualize the results with boxplots to compare group means.\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating data for three groups\n",
        "group1 = np.random.normal(loc=50, scale=10, size=30)\n",
        "group2 = np.random.normal(loc=55, scale=10, size=30)\n",
        "group3 = np.random.normal(loc=60, scale=10, size=30)\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Visualization with boxplots\n",
        "plt.boxplot([group1, group2, group3], labels=[\"Group 1\", \"Group 2\", \"Group 3\"])\n",
        "plt.title('One-Way ANOVA Test - Group Comparisons')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between the group means.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the group means.\")\n",
        "\n",
        "# Question: Simulate random data from a normal distribution, then perform hypothesis testing to evaluate the means.\n",
        "# Simulating random data\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "\n",
        "# Hypothesis testing: One-sample t-test\n",
        "t_stat, p_value = stats.ttest_1samp(data, 55)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The sample mean is significantly different from 55.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample mean is not significantly different from 55.\")\n",
        "\n",
        "# Question: Perform a hypothesis test for population variance using a Chi-square distribution and interpret the results.\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "sample_variance = np.var(data, ddof=1)\n",
        "\n",
        "# Population variance (assumed)\n",
        "population_variance = 100\n",
        "\n",
        "# Chi-square test for variance\n",
        "chi_square_stat = (len(data) - 1) * sample_variance / population_variance\n",
        "df = len(data) - 1\n",
        "\n",
        "# P-value for Chi-square test\n",
        "p_value = 1 - stats.chi2.cdf(chi_square_stat, df)\n",
        "\n",
        "print(f\"Chi-square statistic: {chi_square_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The sample variance is significantly different from the population variance.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The sample variance is not significantly different from the population variance.\")\n",
        "\n",
        "# Question: Write a Python script to perform a Z-test for comparing proportions between two datasets or groups.\n",
        "success_a = 40\n",
        "total_a = 100\n",
        "success_b = 50\n",
        "total_b = 100\n",
        "\n",
        "# Proportions\n",
        "p_a = success_a / total_a\n",
        "p_b = success_b / total_b\n",
        "\n",
        "# Pooled proportion\n",
        "p_pool = (success_a + success_b) / (total_a + total_b)\n",
        "\n",
        "# Z-test for proportions\n",
        "z_score = (p_a - p_b) / np.sqrt(p_pool * (1 - p_pool) * (1/total_a + 1/total_b))\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "\n",
        "print(f\"Z-score: {z_score}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The proportions are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The proportions are not significantly different.\")\n",
        "\n",
        "# Question: Implement an F-test for comparing the variances of two datasets, then interpret and visualize the results.\n",
        "\n",
        "group1 = np.random.normal(loc=50, scale=10, size=100)\n",
        "group2 = np.random.normal(loc=50, scale=15, size=100)\n",
        "\n",
        "# F-test for variances\n",
        "var1 = np.var(group1, ddof=1)\n",
        "var2 = np.var(group2, ddof=1)\n",
        "\n",
        "f_stat = var1 / var2  # F-statistic for comparing variances\n",
        "dfn = len(group1) - 1  # degrees of freedom for group1\n",
        "dfd = len(group2) - 1  # degrees of freedom for group2\n",
        "\n",
        "# P-value for the F-test\n",
        "p_value = 1 - stats.f.cdf(f_stat, dfn, dfd)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Visualization of variances using boxplots\n",
        "plt.boxplot([group1, group2], labels=[\"Group 1\", \"Group 2\"])\n",
        "plt.title('F-test for Variances Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The variances of the two groups are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances of the two groups are not significantly different.\")\n",
        "\n",
        "#Question: Perform a Chi-square test for goodness of fit with simulated data and analyze the results.\n",
        "observed = np.array([12, 8, 9, 11, 10, 10])  # Observed frequencies for each die face (1-6)\n",
        "expected = np.array([10, 10, 10, 10, 10, 10])  # Expected frequencies for a fair die\n",
        "\n",
        "# Chi-square test for goodness of fit\n",
        "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
        "\n",
        "print(f\"Chi-square statistic: {chi2_stat}, P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: The observed frequencies significantly differ from the expected frequencies.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The observed frequencies do not significantly differ from the expected frequencies.\")\n",
        "\n"
      ]
    }
  ]
}