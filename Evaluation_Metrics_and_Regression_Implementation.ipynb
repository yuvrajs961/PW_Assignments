{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T7PVp-dmm71X"
      },
      "outputs": [],
      "source": [
        "#Theoretical\n",
        "# Question 1: What does R-squared represent in a regression model?\n",
        "# Answer:\n",
        "# R-squared (R²) measures the proportion of variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 0 means no variance is explained, and 1 means perfect explanation. Higher values indicate a better fit.\n",
        "\n",
        "# Question 2: What are the assumptions of linear regression?\n",
        "# Answer:\n",
        "# - Linearity: Relationship between dependent and independent variables is linear.\n",
        "# - Independence: Observations are independent of each other.\n",
        "# - Homoscedasticity: Constant variance of residuals.\n",
        "# - Normality: Residuals are normally distributed.\n",
        "# - No multicollinearity: Independent variables are not highly correlated.\n",
        "\n",
        "# Question 3: What is the difference between R-squared and Adjusted R-squared?\n",
        "# Answer:\n",
        "# R-squared increases with the number of variables, even if they are irrelevant. Adjusted R-squared accounts for the number of predictors, and only increases if the new predictor improves the model meaningfully.\n",
        "\n",
        "# Question 4: Why do we use Mean Squared Error (MSE)?\n",
        "# Answer:\n",
        "# MSE measures the average squared difference between actual and predicted values. It penalizes larger errors more than smaller ones, helping to identify models that make large prediction errors.\n",
        "\n",
        "# Question 5: What does an Adjusted R-squared value of 0.85 indicate?\n",
        "# Answer:\n",
        "# It means that 85% of the variance in the dependent variable is explained by the independent variables, adjusted for the number of predictors. It suggests a strong model fit.\n",
        "\n",
        "# Question 6: How do we check for normality of residuals in linear regression?\n",
        "# Answer:\n",
        "# - Use a Q-Q plot\n",
        "# - Histogram of residuals\n",
        "# - Shapiro-Wilk or Kolmogorov-Smirnov test\n",
        "\n",
        "# Question 7: What is multicollinearity, and how does it impact regression?\n",
        "# Answer:\n",
        "# Multicollinearity occurs when independent variables are highly correlated. It inflates standard errors, making coefficients unreliable and reducing interpretability.\n",
        "\n",
        "# Question 8: What is Mean Absolute Error (MAE)?\n",
        "# Answer:\n",
        "# MAE is the average of absolute differences between predicted and actual values. It gives equal weight to all errors and is less sensitive to outliers than MSE.\n",
        "\n",
        "# Question 9: What are the benefits of using an ML pipeline?\n",
        "# Answer:\n",
        "# - Streamlines preprocessing and modeling\n",
        "# - Reduces risk of data leakage\n",
        "# - Makes code cleaner and reproducible\n",
        "# - Facilitates model deployment and tuning\n",
        "\n",
        "# Question 10: Why is RMSE considered more interpretable than MSE?\n",
        "# Answer:\n",
        "# RMSE is the square root of MSE, which brings the error metric back to the same unit as the target variable, making it more interpretable.\n",
        "\n",
        "# Question 11: What is pickling in Python, and how is it useful in ML?\n",
        "# Answer:\n",
        "# Pickling is the process of converting a Python object into a byte stream. It allows saving trained ML models to disk so they can be reloaded later for predictions.\n",
        "\n",
        "# Question 12: What does a high R-squared value mean?\n",
        "# Answer:\n",
        "# It means a large proportion of the variance in the dependent variable is explained by the independent variables. However, high R² doesn't guarantee a good model if assumptions are violated.\n",
        "\n",
        "# Question 13: What happens if linear regression assumptions are violated?\n",
        "# Answer:\n",
        "# - Inaccurate predictions\n",
        "# - Invalid statistical inference (e.g., p-values)\n",
        "# - Biased or inefficient estimators\n",
        "\n",
        "# Question 14: How can we address multicollinearity in regression?\n",
        "# Answer:\n",
        "# - Remove or combine correlated variables\n",
        "# - Use dimensionality reduction (e.g., PCA)\n",
        "# - Use regularization (e.g., Ridge, Lasso)\n",
        "\n",
        "# Question 15: How can feature selection improve model performance in regression analysis?\n",
        "# Answer:\n",
        "# - Reduces overfitting\n",
        "# - Simplifies the model\n",
        "# - Improves interpretability\n",
        "# - Reduces training time\n",
        "\n",
        "# Question 16: How is Adjusted R-squared calculated?\n",
        "# Answer:\n",
        "# Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - p - 1)]\n",
        "# Where n = number of observations, p = number of predictors\n",
        "\n",
        "# Question 17: Why is MSE sensitive to outliers?\n",
        "# Answer:\n",
        "# MSE squares the errors, so large errors (from outliers) contribute disproportionately more to the total, making the metric sensitive to outliers.\n",
        "\n",
        "# Question 18: What is the role of homoscedasticity in linear regression?\n",
        "# Answer:\n",
        "# Homoscedasticity ensures that the variance of residuals is constant. Violating it (heteroscedasticity) can lead to inefficient and biased estimates.\n",
        "\n",
        "# Question 19: What is Root Mean Squared Error (RMSE)?\n",
        "# Answer:\n",
        "# RMSE is the square root of the Mean Squared Error. It provides a measure of the average magnitude of error, in the same units as the dependent variable.\n",
        "\n",
        "# Question 20: Why is pickling considered risky?\n",
        "# Answer:\n",
        "# - Pickled files can execute arbitrary code when unpickled (security risk)\n",
        "# - Not cross-language compatible\n",
        "# - Not always backward-compatible across Python versions\n",
        "\n",
        "# Question 21: What alternatives exist to pickling for saving ML models?\n",
        "# Answer:\n",
        "# - `joblib` (better for large NumPy arrays)\n",
        "# - `ONNX` (cross-platform model format)\n",
        "# - `PMML` (Predictive Model Markup Language)\n",
        "# - Saving model weights/params manually (e.g., JSON, HDF5)\n",
        "\n",
        "# Question 22: What is heteroscedasticity, and why is it a problem?\n",
        "# Answer:\n",
        "# Heteroscedasticity means non-constant variance of residuals. It violates regression assumptions, leading to inefficient estimates and unreliable confidence intervals.\n",
        "\n",
        "# Question 23: How can interaction terms enhance a regression model's predictive power?\n",
        "# Answer:\n",
        "# Interaction terms capture combined effects of two or more variables on the target. This adds flexibility and allows the model to detect relationships that depend on variable combinations.\n",
        "\n",
        "#Practical\n",
        "\n",
        "# Question 1: Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds\" dataset.\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = np.random.rand(100, 2), np.random.rand(100)  # Two features and target\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y - y_pred\n",
        "\n",
        "# Plot residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Question 2: Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model.\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = np.random.rand(100, 2), np.random.rand(100)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate MSE, MAE, RMSE\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the results\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Question 3: Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity.\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = np.random.rand(100, 2), np.random.rand(100)\n",
        "\n",
        "# Check linearity\n",
        "plt.scatter(X[:, 0], y)\n",
        "plt.title(\"Linearity Check (X1 vs y)\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict and calculate residuals\n",
        "y_pred = model.predict(X)\n",
        "residuals = y - y_pred\n",
        "\n",
        "# Homoscedasticity Check (Residuals plot)\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.title(\"Homoscedasticity Check\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix for multicollinearity\n",
        "corr_matrix = np.corrcoef(X.T)\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Question 4: Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = np.random.rand(100, 2), np.random.rand(100)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with standard scaling and a linear regression model\n",
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = pipeline_lr.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "print(f\"Linear Regression MSE: {mse_lr}\")\n",
        "\n",
        "# Add Ridge regression to the pipeline and evaluate\n",
        "pipeline_ridge = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', Ridge())\n",
        "])\n",
        "\n",
        "# Fit the Ridge pipeline\n",
        "pipeline_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_ridge = pipeline_ridge.predict(X_test)\n",
        "\n",
        "# Evaluate Ridge performance\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "print(f\"Ridge Regression MSE: {mse_ridge}\")\n",
        "\n",
        "# Question 5: Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Model Coefficient: {model.coef_[0]}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {model.score(X, y)}\")\n",
        "\n",
        "# Question 6: Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using simple linear regression and visualizes the results.\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Prepare the data\n",
        "X = tips[['total_bill']]  # Independent variable\n",
        "y = tips['tip']  # Dependent variable\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression Line\")\n",
        "plt.title(\"Total Bill vs Tip\")\n",
        "plt.xlabel(\"Total Bill\")\n",
        "plt.ylabel(\"Tip\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 7: Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10  # Feature\n",
        "y = 3 * X + np.random.randn(100, 1) * 5  # Target with some noise\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot data and regression line\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression line\")\n",
        "plt.title(\"Linear Regression with Synthetic Data\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 8: Write a Python script that pickles a trained linear regression model and saves it to a file.\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X + np.random.randn(100, 1) * 5\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Pickle the trained model\n",
        "with open(\"linear_regression_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model has been pickled and saved.\")\n",
        "\n",
        "# Question 9: Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve.\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = X**2 + np.random.randn(100, 1) * 10  # Quadratic relation with noise\n",
        "\n",
        "# Transform data for polynomial regression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data and regression curve\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial regression line\")\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 10: Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Model Coefficient: {model.coef_[0]}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {model.score(X, y)}\")\n",
        "\n",
        "# Question 11: Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = X**3 + np.random.randn(100, 1) * 50  # Cubic relationship with noise\n",
        "\n",
        "# List of polynomial degrees to try\n",
        "degrees = [1, 2, 3, 4]\n",
        "\n",
        "# Plotting the data and polynomial regression curves\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "\n",
        "for degree in degrees:\n",
        "    # Transform the data for polynomial regression\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    # Fit the model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "\n",
        "    # Predict and plot the regression curve\n",
        "    y_pred = model.predict(X_poly)\n",
        "    plt.plot(X, y_pred, label=f\"Degree {degree}\")\n",
        "\n",
        "    # Print the performance metrics\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    print(f\"Degree {degree} MSE: {mse:.2f}\")\n",
        "\n",
        "# Plot customization\n",
        "plt.title(\"Polynomial Regression for Different Degrees\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 12: Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data with two features\n",
        "X, y = make_regression(n_samples=100, n_features=2, noise=10.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {model.score(X, y)}\")\n",
        "\n",
        "# Question 13: Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + np.random.randn(100, 1) * 2  # Linear relationship with noise\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data and regression line\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression line\")\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 14: Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3) * 10\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] + 4 * X[:, 2] + np.random.randn(100) * 5\n",
        "\n",
        "# Convert X to a DataFrame\n",
        "X_df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3'])\n",
        "\n",
        "# Add constant for VIF calculation\n",
        "X_with_const = add_constant(X_df)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_with_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "\n",
        "print(vif_data)\n",
        "\n",
        "# Question 15: Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data (degree 4)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = X**4 + np.random.randn(100, 1) * 100  # Polynomial relationship with noise\n",
        "\n",
        "# Transform data for polynomial regression\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data and regression curve\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial regression line\")\n",
        "plt.title(\"Polynomial Regression (Degree 4)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 16: Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score.\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data for multiple regression\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Create a pipeline with standardization and linear regression\n",
        "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"R-squared Score: {pipeline.score(X, y)}\")\n",
        "\n",
        "# Question 17: Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data (degree 3)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = X**3 + np.random.randn(100, 1) * 100  # Cubic relationship with noise\n",
        "\n",
        "# Transform data for polynomial regression\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data and regression curve\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial regression curve\")\n",
        "plt.title(\"Polynomial Regression (Degree 3)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 18: Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data with 5 features\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {model.score(X, y)}\")\n",
        "\n",
        "# Question 19: Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X + np.random.randn(100, 1) * 5  # Linear relationship with noise\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict the values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data and regression line\n",
        "plt.scatter(X, y, color='blue', label=\"Data points\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Regression line\")\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Question 20: Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=10.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared Score: {model.score(X, y)}\")\n",
        "\n",
        "# Question 21: Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling.\n",
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Train a model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(model, 'linear_regression_model.joblib')\n",
        "\n",
        "# Load the model from file\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n",
        "\n",
        "# Print R-squared score of the loaded model\n",
        "print(f\"R-squared score from loaded model: {loaded_model.score(X, y)}\")\n",
        "\n",
        "# Question 22: Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset.\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define features (categorical and continuous) and target\n",
        "X = tips[['sex', 'day', 'time', 'size']]\n",
        "y = tips['total_bill']\n",
        "\n",
        "# Define preprocessing pipeline (one-hot encode categorical features)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(), ['sex', 'day', 'time']),\n",
        "        ('num', 'passthrough', ['size'])\n",
        "    ])\n",
        "\n",
        "# Define full pipeline (preprocessing + linear regression)\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Print R-squared score on the test set\n",
        "print(f\"R-squared score: {pipeline.score(X_test, y_test)}\")\n",
        "\n",
        "# Question 23: Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R-squared score.\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Train Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X, y)\n",
        "\n",
        "# Train Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X, y)\n",
        "\n",
        "# Print results for Linear Regression\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"Coefficients: {lr_model.coef_}\")\n",
        "print(f\"Intercept: {lr_model.intercept_}\")\n",
        "print(f\"R-squared Score: {lr_model.score(X, y)}\")\n",
        "\n",
        "# Print results for Ridge Regression\n",
        "print(\"\\nRidge Regression:\")\n",
        "print(f\"Coefficients: {ridge_model.coef_}\")\n",
        "print(f\"Intercept: {ridge_model.intercept_}\")\n",
        "print(f\"R-squared Score: {ridge_model.score(X, y)}\")\n",
        "\n",
        "# Question 24: Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation with 5 folds\n",
        "cross_val_scores = cross_val_score(lr_model, X, y, cv=5)\n",
        "\n",
        "# Print cross-validation results\n",
        "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
        "print(f\"Mean cross-validation score: {cross_val_scores.mean()}\")\n",
        "\n",
        "# Question 25: Write a Python script that compares polynomial regression models of different degrees and prints the R-squared score for each.\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data for polynomial regression\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = X**3 + np.random.randn(100, 1) * 100  # Cubic relationship with noise\n",
        "\n",
        "# Try different polynomial degrees and compare R-squared scores\n",
        "for degree in [1, 2, 3, 4]:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    # Fit the model\n",
        "    poly_model = LinearRegression()\n",
        "    poly_model.fit(X_poly, y)\n",
        "\n",
        "    # Print R-squared score for each degree\n",
        "    print(f\"Degree {degree} Polynomial Regression R-squared: {poly_model.score(X_poly, y)}\")"
      ]
    }
  ]
}