{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T7PVp-dmm71X"
      },
      "outputs": [],
      "source": [
        "#Theory\n",
        "# Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "# Answer:\n",
        "# Logistic Regression is a statistical method used for binary classification tasks.\n",
        "# It predicts the probability that a given input belongs to a certain class (e.g., 0 or 1).\n",
        "# The output is always between 0 and 1, which makes it suitable for classification.\n",
        "# Unlike Linear Regression, which outputs continuous values and fits a straight line,\n",
        "# Logistic Regression uses the sigmoid function to squeeze the output into the [0, 1] range.\n",
        "# Thus, Linear Regression is used for regression tasks, while Logistic Regression is used for classification.\n",
        "\n",
        "# Question 2: What is the mathematical equation of Logistic Regression?\n",
        "# Answer:\n",
        "# The hypothesis of Logistic Regression is given by:\n",
        "# hθ(x) = 1 / (1 + e^(-θᵀx))\n",
        "# Here, θ is the vector of model parameters, x is the feature vector,\n",
        "# and hθ(x) gives the probability that the output is 1.\n",
        "\n",
        "# Question 3: Why do we use the Sigmoid function in Logistic Regression?\n",
        "# Answer:\n",
        "# The sigmoid function maps any real-valued number into a value between 0 and 1,\n",
        "# which is interpreted as a probability.\n",
        "# It allows us to convert the linear combination θᵀx into a probability score.\n",
        "# This is essential for binary classification problems.\n",
        "\n",
        "# Question 4: What is the cost function of Logistic Regression?\n",
        "# Answer:\n",
        "# The cost function is the log loss or cross-entropy loss, defined as:\n",
        "# J(θ) = -1/m ∑ [y log(hθ(x)) + (1 - y) log(1 - hθ(x))]\n",
        "# This function penalizes wrong predictions more heavily and is convex, which is good for optimization.\n",
        "\n",
        "# Question 5: What is Regularization in Logistic Regression? Why is it needed?\n",
        "# Answer:\n",
        "# Regularization is a technique to prevent overfitting by adding a penalty term to the cost function.\n",
        "# It discourages the model from learning overly complex or large parameter values.\n",
        "# This helps in improving generalization to unseen data.\n",
        "\n",
        "# Question 6: Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "# Answer:\n",
        "# - Lasso (L1 regularization): Adds λ∑|θj| to the cost. Can shrink some coefficients to zero, effectively selecting features.\n",
        "# - Ridge (L2 regularization): Adds λ∑θj² to the cost. Shrinks coefficients but doesn’t eliminate them.\n",
        "# - Elastic Net: Combines both L1 and L2 penalties. Useful when there are multiple correlated features.\n",
        "\n",
        "# Question 7: When should we use Elastic Net instead of Lasso or Ridge?\n",
        "# Answer:\n",
        "# Use Elastic Net when:\n",
        "# - You have many correlated features.\n",
        "# - Lasso eliminates too many features.\n",
        "# - Ridge retains too many irrelevant features.\n",
        "# Elastic Net provides a balance between feature selection and coefficient shrinkage.\n",
        "\n",
        "# Question 8: What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "# Answer:\n",
        "# - A higher λ increases regularization, shrinking coefficients more and reducing overfitting but may lead to underfitting.\n",
        "# - A lower λ reduces regularization, allowing the model to fit the training data more closely, increasing the risk of overfitting.\n",
        "\n",
        "# Question 9: What are the key assumptions of Logistic Regression?\n",
        "# Answer:\n",
        "# - The dependent variable is binary.\n",
        "# - Observations are independent.\n",
        "# - There is little to no multicollinearity.\n",
        "# - The relationship between independent variables and the log-odds is linear.\n",
        "# - Large sample sizes for stability.\n",
        "\n",
        "# Question 10: What are some alternatives to Logistic Regression for classification tasks?\n",
        "# Answer:\n",
        "# - Decision Trees\n",
        "# - Random Forest\n",
        "# - Support Vector Machines\n",
        "# - k-Nearest Neighbors\n",
        "# - Naive Bayes\n",
        "# - Gradient Boosting (e.g., XGBoost)\n",
        "# - Neural Networks\n",
        "\n",
        "# Question 11: What are Classification Evaluation Metrics?\n",
        "# Answer:\n",
        "# - Accuracy\n",
        "# - Precision\n",
        "# - Recall\n",
        "# - F1 Score\n",
        "# - ROC-AUC\n",
        "# - Confusion Matrix\n",
        "# - Log Loss\n",
        "\n",
        "# Question 12: How does class imbalance affect Logistic Regression?\n",
        "# Answer:\n",
        "# Class imbalance can cause the model to be biased toward the majority class,\n",
        "# resulting in poor recall and precision for the minority class.\n",
        "# Techniques like resampling, using class weights, or alternative metrics like ROC-AUC help handle this.\n",
        "\n",
        "# Question 13: What is Hyperparameter Tuning in Logistic Regression?\n",
        "# Answer:\n",
        "# It’s the process of finding the best values for parameters like regularization strength (λ),\n",
        "# solver type, or max iterations using methods like Grid Search or Random Search with cross-validation.\n",
        "\n",
        "# Question 14: What are different solvers in Logistic Regression? Which one should be used?\n",
        "# Answer:\n",
        "# Common solvers:\n",
        "# - liblinear: Good for small datasets and L1 regularization.\n",
        "# - lbfgs: Suitable for multiclass and large datasets.\n",
        "# - saga: Supports L1, L2, and Elastic Net; works with large data.\n",
        "# - newton-cg: Suitable for L2 regularization and large datasets.\n",
        "# Choose based on data size, regularization type, and computational resources.\n",
        "\n",
        "# Question 15: How is Logistic Regression extended for multiclass classification?\n",
        "# Answer:\n",
        "# - One-vs-Rest (OvR): Trains a separate classifier for each class vs. all others.\n",
        "# - Multinomial Logistic Regression (Softmax): Generalizes logistic regression to handle multiple classes in one model using the softmax function.\n",
        "\n",
        "# Question 16: What are the advantages and disadvantages of Logistic Regression?\n",
        "# Answer:\n",
        "# Advantages:\n",
        "# - Simple and fast\n",
        "# - Probabilistic output\n",
        "# - Works well with linearly separable data\n",
        "# Disadvantages:\n",
        "# - Assumes linearity in log-odds\n",
        "# - Not effective for complex relationships\n",
        "# - Sensitive to outliers and multicollinearity\n",
        "\n",
        "# Question 17: What are some use cases of Logistic Regression?\n",
        "# Answer:\n",
        "# - Spam detection\n",
        "# - Disease diagnosis\n",
        "# - Credit scoring\n",
        "# - Customer churn prediction\n",
        "# - Fraud detection\n",
        "\n",
        "# Question 18: What is the difference between Softmax Regression and Logistic Regression?\n",
        "# Answer:\n",
        "# - Logistic Regression: Used for binary classification.\n",
        "# - Softmax Regression: Used for multiclass classification.\n",
        "# It assigns probabilities to each class using the softmax function, summing to 1.\n",
        "\n",
        "# Question 19: How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "# Answer:\n",
        "# - Use OvR for small datasets or binary classifiers with good performance.\n",
        "# - Use Softmax (multinomial) when classes are balanced and mutually exclusive.\n",
        "# It is often more accurate for true multiclass settings.\n",
        "\n",
        "# Question 20: How do we interpret coefficients in Logistic Regression?\n",
        "# Answer:\n",
        "# Each coefficient θj represents the change in the log-odds of the outcome for a one-unit increase in the corresponding feature xj.\n",
        "# Exponentiating θj gives the odds ratio:\n",
        "# e^(θj) = multiplicative change in odds\n",
        "# Values >1 increase odds; values <1 decrease odds.\n",
        "\n",
        "\n",
        "#Practical\n",
        "\n",
        "# Question 1: Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "# Answer:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "# Predict and print accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Question 2: Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "# Answer:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with L1 regularization:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#Question 3: Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "#Answer:\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with L2 regularization:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "#Question 4: Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "#Answer:\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with Elastic Net:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#Question 5: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "#Answer:\n",
        "from sklearn.datasets import load_digits\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass Accuracy (OvR):\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Question 6: Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "#Answer:\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2\n",
        "}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Question 7: Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "#Answer:\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Stratified K-Fold Accuracy Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n",
        "\n",
        "# Question 8: Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "# Answer:\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "df = pd.read_csv('your_dataset.csv')  # Replace with actual path\n",
        "X = df.drop('target', axis=1)\n",
        "y = LabelEncoder().fit_transform(df['target'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "# Question 9: Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "# Answer:\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=10000), param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Question 10: Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "Answer:\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"One-vs-One Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#Question 11: Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "#Answer:\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "#Question 12: Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "#Answer:\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "#Question 13: Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "#Answer:\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with Class Weights:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#Question 14: Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "#Answer:\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "df = df[['sex', 'age', 'fare', 'embarked', 'survived']].dropna()\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df['embarked'] = df['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Titanic Data Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "#Question 15: Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "#Answer:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Without Scaling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "acc_no_scaling = accuracy_score(y_test, model.predict(X_test))\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", acc_scaled)\n",
        "\n",
        "#Question 16: Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "#Answer:\n",
        "from sklearn.metrics import roc_auc_score\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "#Question 17: Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "#Answer:\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#Question 18: Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "#Answer:\n",
        "import numpy as np\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "importance = np.abs(model.coef_[0])\n",
        "for i, v in enumerate(importance):\n",
        "    print(f\"Feature {i}: Coefficient = {model.coef_[0][i]}\")\n",
        "\n",
        "#Question 19: Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "#Answer:\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n",
        "\n",
        "#Question 20: Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "#Answer:\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nu9oFG6gnBlM"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}